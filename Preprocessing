import spacy
import pandas as pd
import re
from gensim.models.phrases import Phrases, Phraser
from gensim.models import Word2Vec
import logging
##This script processes COVID-19 vaccine-related tweets, obtainted from Kaggle, and utilizes Word2Vec to convert words to vectors for input into clustering model


logging.basicConfig(format="%(levelname)s - %(asctime)s: %(message)s", datefmt= '%H:%M:%S', level=logging.INFO)


path = '/Users/evanhu/Documents/vaccination_tweets.csv'
vaccine_tweets = pd.read_csv(path)

##filter out only columns of interest
vaccine_tweets_1 = vaccine_tweets[['id', 'user_name', 'user_location', 'date', 'text']].copy()
vaccine_tweets_1 = vaccine_tweets.dropna().reset_index(drop=True)


nlp = spacy.load("en_core_web_sm")

def cleaning(doc):
    ##remove stop words and lemmatize
    text = [token.lemma_ for token in doc if not token.is_stop]
    if len(text) > 2:
        return ' '.join(text)

##removes nonletters
cleaning_remove_nonletters = (re.sub("[^A-Za-z']+", ' ', str(element)).lower() for element in vaccine_tweets_1['text'])
text_clean = [cleaning(doc) for doc in nlp.pipe(cleaning_remove_nonletters, batch_size=500, n_threads=-1)]

##creates new dataframe using clean data
df_clean = pd.DataFrame({'clean': text_clean})
df_clean = df_clean.dropna().drop_duplicates()

sentence = [element.split() for element in df_clean['clean']]
phrases = Phrases(sentence, min_count=1, progress_per=500)
bigram = Phraser(phrases)
sentences = bigram[sentence]

##Vectorization
model = Word2Vec(min_count=3,
                     window=3,
                     size=300,
                     sample=6e-5,
                     alpha=0.03,
                     min_alpha=0.0007,
                     negative=20,
                    )

model.build_vocab(sentences, progress_per=500)
model.train(sentences, total_examples=model.corpus_count, epochs=30, report_delay=1)
model.init_sims(replace=True)
model.save("/Users/evanhu/Documents/word2vec.model")










